\chapter{Introduction \& Motivation}

\section{Why Develop the Use of Sonification for Affective Computing?}
In this section I will talk about why we should try to develop sonification strategies for affective display

\begin{enumerate}
\item Use when visual or verbal attention is already occupied:
	\begin{enumerate}
	\item When interacting with a visual interface
	\item When interacting with another human
	\end{enumerate}
\item Use to show bodily-based measures of affect that are not socially communicated (e.g. Electrodermal Activity)
\item Emotional Communication benefits from diversity and redundancy (best to have as many cues as possible for conveying emotion)
\item There is a need for continuous display of arousal and valence (that is, there are technologies that can produce continuous display of arousal and valence).
\item Non-speech audio seems remarkably capable of emotional communication using simple psychoacoustic cues
\end{enumerate}

\subsection{When Visual or Verbal Attention is already occupied}
\section{Affective Display}

In the literature on affective computing, most articles are written on emotion recognition or modelling.  Technologies for displaying emotional information or mediating emotional communication however are very important as they form the basis of meaningful communication with the human agent.  There are many ways of displaying emotional information.  Popular and well-researched methods use facial display, gesture or postural display, or speech prosody.  These are very effective mechanisms and highly useful.  

One disadvantage of these is that they require visual or verbal attention, so in situations where visual or verbal attention is already occupied, their use adds significantly to the cognitive load.  Another related limitation is their requirements for display.  For instance, displaying synthesized faces or gestures to communicate emotion require a visual monitor or in the case of robotics, a face or a robotic-body.  In the case of using speech for emotional communication, there is a requirement of a voice or speech for transmission.  When their is no linguistic content to be conveyed, synthesized speech is not useful.

The benefits of sound is that it can be used to monitor emotions in an eyes and speech-free fashion.  Using sound, one can use the eyes to simultaneously interact with a user interface or another human.  Similarly, one can listen to another human speaking while still being able to monitor emotions. 

\section{Why Use ``Emotional" Sounds instead of Abstract Sounds? (Sonification, Auditory Icons or Earcons?)}

In sonification, often there is no clear correspondence between data and sound a priori, so an important part of the mapping is creating a metaphor that can be quickly understood by the listener but also used to hear out subtle details so that it can be used as a data bearing medium.  The problem when using sonification for display of arousal and valence, as is being discussed presently, is that the underlying data may be more accurately presented using a mapping that \textit{doesn't} use emotional cues.  Furthermore, another mapping strategy may create a more convincing metaphor  (Give an example, maybe a hammer on a piece of wood).  So why use psychoacoustic cues determined by emotional studies?

\begin{description}
\item[Clear mappi	ng/universality] One doesn't need to explain what to listen for in the mapping.  A sound just is "happy" or "sad."
\item[``Low-Level Processing"] Just as you can listen to music in a movie and have it influence your emotions w/o having to pay attention, a sonification mapping strategy consistent w/ these cues can be analyzed on a lower level.
\item[Applies Experimental Predictions] Although there is a vast literature on auditory perception, parameter mapping sonification often applies complex auditory models for data display, each context applying different models, and in general there is no consensus on the best way to do mapping, though many ways have been proposed.  Unlike in most cases, the literature on the psychoacoustic elicitors of emotion is huge and goes back three-quarters of a century.  Further, there are now computational models that predict listeners responses to music from these low-level features (Cite Coutinho 2009).  It just makes sense to draw from a literature that is already so huge.
\item[Contributes to Emotion Research]  Developing these sorts of models provides music researchers with acoustic models that use the features they predict to convey emotion, but do not have the associations with music that music has.  Therefore, it would be reasonably impossible for a listener to ``recognize" a sonification as they could recognize a piece of music.  Therefore this sort of emotional induction would be impossible.  Same thing with Juslin's other non-acoustic mechanisms.  Abstracting into a range of just psychoacoustic features provides a level of ``cleanness" to experimental stimuli.    


\end{description}


%\section{Affective Computing}

%\section{Problem Statement}

%\section{Contributions}

%\section{Thesis Overview}


































