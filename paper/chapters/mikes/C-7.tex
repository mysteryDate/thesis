\chapter{Conclusions and Future Work}

This thesis addressed a number of key challenges related to automatically building 3D models of urban scenes using mobile 
LiDAR and images. With a state-of-the-art acquisition system, Navteq True, I showed that relatively good results could be obtained by using fairly general assumptions. However, Navteq True is an expensive technology, which limited its accessibility to wider community in both academia and industry. 
%The notion of a virtual sensor has been around for a while, but has been recently popularized by systems such as Microsoft Photosynth ~\cite{photosynth-url} that use ensembles of data collected from multiple sensors to build very compelling representations of urban scenes.
In this thesis, I considered a virtual sensor in terms of a sparse LiDAR sensor coupled with moderate resolution images.  Although I did not use a physical sensor in this instance, I was able to simulate its characteristics by appropriately sampling Navteq True, which ultimately provided reliable ground truth. 
I have accomplished most of the way to achieve the goal,
leaving a number of challenges for future research.

I began, in Chapter 3, by considering the ill-posed problem of
how much 3D information one can infer from single images. I developed a new model-based method to reconstruct the dimensions of rectilinear buildings from single images, which does not require a model-to-image projection and readjustment procedures as existing methods do \cite{Lowe91, Taylor95, Debevec96}. With this method, one can perform accurate 3D measurement using a single image taken from an off-the-shelf camera without using any vanishing points or ground control points, and the results are more accurate than vanishing point based methods ($e.g.,$ \cite{ZhangZ01}). The building topology can be inferred under a flat terrain assumption. I demonstrated that the simple building models can be reconstructed from single images. To reconstruct the
detailed geometry of the scene, we rely on the state-of-the-art mobile mapping system Navteq True which is capable of
collecting dense LiDAR and panoramic images at ground
level. 

In Chapter 4, I worked on the data collected from NAVTEQ True,
and tackled the difficult problem of window detection from the noisy LiDAR data, which is is an important step towards photorealistic urban modeling. I proposed a strategy which combines a bottom-up with a top-down scheme to extract building facades from noisy LiDAR data, and a robust window detection algorithm. The results
appear to be competitive with the state of the art, but without  limitations imposed by the need to reconstruct the scene or rely
on image-based (2D) approaches. Computational efficiency and accurate localization are essential in order to operate on the large
datasets associated with metropolitan regions. With this advanced data acquisition technology, I showed that the detailed structure of buildings such as windows can be extracted correctly. However, NAVTEQ True is an expensive technology, can we get by with less? To answer this question,  I addressed {\it virtual sensor} problem in the context of mobile urban mapping. To do so,
we need to solve the multi-modal registration problem described in Chapter 5 and interpolate up the sparse data described in Chapter 6. 

In Chapter 5, I registered 2D panoramic images with 3D LiDAR data. The existing method \cite{MastinFisher09} does not work on our ground level data. We use maximization of mutual information
for the registration, and designed an algorithm that can automatically run through the mobile LiDAR and spherical panoramas data collected over a large metropolitan area. It is the first example we are aware of that tests mutual information registration in a large-scale context.  
The registration between LiDAR and images facilitates the following upsampling methods, which then make it possible to apply the successful approach developed for Navteq True to off-the-shelf
technologies.

In Chapter 6, I first developed a new point visibility computation method based on depth maps generated from a Quadrilateralized Spherical Cube mapping. Then I integrated point visibility information into the upsampling procedure that existing methods do not consider, but is a common problem existing in range data. I proposed a new interpolation scheme based on a ray casting method, and demonstrated the improvement of upsampling by using color information of the spherical images. Finally, we validated the {\it virtual sensor} concept, showing the strengths and limitations of the approach in the context of window detection.
 
\section{Future Work}

In the image-based method, I reconstructed the accurate dimensions of rectilinear buildings provided that one dimension of the building is known. This approach can be easily extended to deal with an object that can be modeled as a polyhedron, where the coordinates of the vertices can be expressed as a linear function of a parameter vector. Second, the
prior information about one dimension of the building is employed to determine the scale as single view modeling only recovers a scene up to a scale factor without any prior information. There are many alternative ways to determine the scale, $e.g.,$ place a ruler in the scene where real length is known, use architectural design information such as the standard dimensions for windows or doors in buildings etc. By using these alternatives, our method can have less constraints, $e.g.,$ without the need of knowing one building dimension. Third, the manual correspondence between image and model edges may introduce errors. An improvement can be done by
integrating an edge detection in the areas specified by the user to obtain more accurate edges for the correspondence. 

In the LiDAR based reconstruction, we separated the ground floor from the facade manually, which can be automated through symmetry pattern analysis. Another possible direction is to integrate images with LiDAR for better window detection as images have higher resolution and more visual information than LiDAR. The edge detection from high resolution images can provide more accurate boundary information which can increase the window detection accuracy. The buildings used for window detection in this thesis are all separated individual buildings. In urban scene, a quite common situation is that
many buildings with different heights or patterns are connected together. Efficient methods are needed to separate these connected buildings into individual buildings for a better symmetry pattern analysis. 
 
In the image-to-range registration, we generated perspective images from spherical images using the view either perpendicular or parallel to the vehicle driving direction. Therefore, we just used 1/6 of the entire spherical image for the mutual information registration, which does not efficiently use all the available information contained in the 360$^\circ$ panoramic images. One possible approach is to project the entire LiDAR dataset along with spherical images onto 6 cube faces using a Quadrilateralized Spherical Cube mapping \cite{ChanNeill75} or other linear projections. Because the sky and the ground do not provide much useful information, we actually just need 4 faces for the mutual information registration. To speed up the computation, a multi-resolution approach can be employed. Using multi-resolution techniques for image registration is a widely used approach to improve speed, accuracy, and robustness of the registration algorithm. The registration is first performed at a coarse level where the images have fewer pixels. The results from the coarse level are then used to initialize registration at the next finer level. This process is repeated until it reaches the finest level. This coarse-to-fine strategy improves the performance of the registration algorithm and also increases robustness by eliminating local optima at coarser level. 

One of the limitations of MI metric is that the intensity histograms contain no spatial information. One possible direction is to incorporate spatial context information into the metric to improve the robustness of the similarity measure. Another possible direction is to improve performance of image-to-range registration without the use of the initial camera poses which are normally from GPS/IMU solutions. This probably will involve more sophisticated optimization algorithms overcoming many local minima, or using different registration approaches. The method in this chapter can also be extended to deal with other images of different modalities such as synthetic aperture radar or infrared imagery, provided that proper LiDAR data are available.

In the upsampling method, we solved the upsampling method based on a deterministic formulation of the problem. There are certainly some aspects of the algorithm where the improvements can be made. For instance, we solved the problem of outlier removal in the depth map using a simple model. In practice, the desired algorithm should be able to intelligently differentiate foreground and background, and identify whether points are outliers or foreground by statistical modeling methods. This would be very helpful for improving visibility computation. Second, the strategy of our interpolation is a uniform sampling in 2D space but a non-uniform one in 3D space. We can also improve current results by using a uniform sampling in 3D space. To do so, we need to project each relevant region boundary to 3D space, and form a 3D bounding box for each projected boundary. A uniform sampling can then be performed by choosing an appropriate resolution in 3D space. Third, we can relax the planar structure restriction and assume that each segmented region corresponds to a smooth 3D surface. By doing so, we hope that the RP method will be applicable to more situations where the planar assumption is violated. We also plan to incorporate color information in the RT method. Here the idea is to use the segmentation boundary to aid in building a constrained 2D Delaunay Triangulation on the projected 2D LiDAR points. Upsampling based on a constrained Delaunay Triangulation would effectively remove erroneous interpolations as shown in Figure \ref{fig:C6:lidar1RT} and \ref{fig:C6:lidar3RT}. We hope that this will increase the accuracy of the RT method. The image segmentation method needs to be improved too, especially in presence of highly textured surfaces. Segmentation methods incorporating depth and texture information can be employed to avoid an over-segmentation effect, which will helps the interpolation. The better interpolation will make it
possible to extract features from sparse LiDAR data, so that
the use of a cost-effective solution will be comparable to an expensive one, ultimately. Another research direction would be using a pair or more spherical images for upsampling range data. The occluded region in the first image may be available in the other images, which will increase the accuracy of the interpolation. Besides, additional spherical images could generate geometric information which can be directly integrated into the range data. 

Eventually, with the techniques described in this thesis, I would like to provide a technology with the fidelity and performance of Navteq True at a more accessible cost, making this technology accessible to the wider Geomatics community.
Our work brings contributions to the field of both computer vision and remote sensing. 
